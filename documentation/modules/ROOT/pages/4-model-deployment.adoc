= 4. モデルのデプロイ - 15 minutes
:imagesdir: ../assets/images

モデルのトレーニング方法とその使用方法の概要を説明した後、OpenShift AI を使用してモデルを提供する方法を見ていきます。これにより、後でインテリジェント アプリケーションで使用しやすくなります。

OpenShift AI は、 https://docs.openvino.ai/latest/ovms_what_is_openvino_model_server.html[Intel の OpenVino Model Server^] ランタイムを統合しています。これは、Intel アーキテクチャでのデプロイメント用に最適化された、モデルを提供するための高性能システムです。

私たちが使用するモデルは、写真内の **T シャツ** 、 **ボトル** 、 **帽子** を分離して認識できる **オブジェクト検出モデル** です。このプロセスは前のセクションで見たものと全体的に同じですが、このモデルは GPU の助けを借りても数時間かかるため、すでにトレーニングされているものを使用します。このトレーニング プロセスについて詳しく知りたい場合は、 https://github.com/rh-aiservices-bu/yolov5-transfer-learning[こちら^]をご覧ください。

モデルは機械学習の相互運用性のオープン標準である https://onnx.ai/[ONNX^] 形式で保存されており、OpenVino および OpenShift AI モデルで使用できます。モデルはAWS S3 バケットからダウンロードできます。

このモデルをデプロイしましょう!

== 4.1. モデルの確認

AWS S3と対話するために利用できるさまざまな方法やツールがあります。ここでは `boto3` という名前の Python ライブラリを使用します。

* 使用するノートブックでは、モデルの保管場所にアクセスするための資格情報を復号化するためのキーを入力する必要があります。このキーは次のとおりです。

[.console-input]
[source,text]
----
Is_34AxyLht603Lh4z6UYztla79lQ3oz_os7U99JsKQ=
----

これをコピーしてください。ノートブックの最初のセルにある `replace_me` プレースホルダーを置き換えるのにすぐに必要になります。

IMPORTANT: キーをコピーするときは、行末の「=」記号を選択することを忘れないでください。

(心配しないでください。アクセスしようとしているバケットは読み取り専用であり、それらのアクセス/シークレットはパブリック モデル ファイルを読み取るためにのみ使用できます。したがって、復号化された資格情報であっても機密性はまったくありません。)

* JupyterLab のプロジェクトで、ノートブック `03_check_model` を開き、指示に従ってモデルの可用性と正確な場所を確認します。繰り返しますが、セルを実行することを忘れないでください。

image::check_model.png[]

* 完了したら、こちらの手順に戻ることができます。

image::check_model-complete.png[]

== 4.2. OpenShift AI にモデルをデプロイ

S3ストレージにあるモデルにアクセスできればデプロイするのは非常に簡単です。

* OpenShift AI ダッシュボードに戻ります。閉じた場合は、 https://rhods-dashboard-redhat-ods-applications.%SUBDOMAIN%[ここ^] にアクセスするか、 **File->Hub Control Panel** をクリックして Jupyter から再度開くことができます。 

image::hub_panel.png[]

* データ サイエンス プロジェクト内にある OpenShift AI ダッシュボードで、データ接続を作成します。これは、オブジェクト バケットにアクセスするための参照および設定として機能します。 **Data connections** セクションで、 **Add data connection** を選択します。

image::add_data_connection.png[]

* 情報を入力し、 **Add data connection** を選択します。
    ** `Coolstore` という名前をつけます。
    ** `Access Key` はノートブックの前の手順で取得した `Access key` です。
    ** `Secret key` はノートブックの前の手順で取得した `Secret key` です。
    ** `Endpoint` は `https://s3.amazonaws.com/` (default) です。
    ** `Region` は `us-east-1` (default) です。
    ** `Bucket` は `rh-mad-workshop-m6` です。

接続されたワークベンチを指定する必要はありません。このデータ接続はモデル サーバーによってのみ使用されます。

image::data_connection_configuration.png[]

* これで、データ接続を使用できるようになりました。

image::data_connection_ok.png[]

* 次に、 **Models and model servers** セクションで **Configure server** を選択します。

image::add_model_server.png[]

* Model server name に *coolstore-modelserver* 入力し、Serving runtime で *OpenVINO Model Server* を選択します。
* replicas は 1 のままにしておき、 size は Smallを選択します。この時点では、 **Make model available via an external route** の `チェックをしない` でください。そして、 **Add**　を選択します。

image::model_server_configure.png[]

* これで、モデル サーバーが利用可能になりました (ただし、モデルはまだデプロイされていません)。 **Deploy model** を選択します。

image::model_server_available.png[]

* 
モデルに `coolstore` という名前を付け、フレームワークとして **onnx - 1** を選択し、モデルの場所として前に作成したデータの場所を選択し、フォルダー パスとして **coolstore-model** を入力します。モデル (先頭の `/` なし) を選択し、**Deploy** を選択します。

image::deploy_model_configuration.png[]

* モデルは現在デプロイ中です。詳細を表示するには、 `Deployed models` タブの `1` をクリックする必要があります。

image::deployed_models.png[]

* しばらくすると、 **Status** 列の下、モデルの右側に緑色のチェックマークが表示され、使用できる状態になったことを示します。

image::model_ready.png[]

* 準備ができたら、 **Inference endpoint** 列の下にある `Internal Service` リンクを選択し、grpcURL 値をメモします。後で必要になります。 次のようになっているはずです。
`grpc://modelmesh-serving.%USERID%-data-science-project:8033`

image::grpcurl.png[]

== 4.3. モデルのテスト

モデル サーバーがリクエストを受信する準備ができたので、テストできます。

* JupyterLab のプロジェクトで、ノートブック `04_remote_inference` を開き、指示に従ってモデルをクエリする方法を確認します。

image::remote_inference.png[]

* ノートの指示を完了すると、次の結果が得られます。

image::remote_inference_complete.png[]
